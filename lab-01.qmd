---
title: "lab-01"
subtitle: 'Public Health & Time: COVID Trends'
author:
  - name: Matthew Woodworth
    email: mattwood@colostate.edu
format:
  html:
    code-fold: true
    toc: true
    include-before-body: ../slides/header.html
    include-after-body:  ../slides/footer-annotations.html
---

```{r, include = F}
knitr::opts_chunk$set(fig.width = 6, 
                      message = FALSE, 
                      warning = FALSE, 
                      comment = "", 
                      cache = FALSE, 
                      fig.retina = 3)

library(zoo)
```


# **Question 1**: Daily Summary

Looking at the README in the NYT repository we read:

> "We are providing two sets of data with cumulative counts of coronavirus cases and deaths: one with our most current numbers for each geography and another with historical data showing the tally for each day for each geography ... the historical files are the final counts at the end of each day ... The historical and live data are released in three files, one for each of these geographic levels: U.S., states and counties."

For this lab we will use the historic, county level data which is stored as a CSV at this URL:

```{r, eval = FALSE}
https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv
```


To start, you should set up a reproducible framework to communicate the following in a way that can be updated every time new data is released (daily):

  1. cumulative cases in the 5 worst counties
  2. total **NEW** cases in the 5 worst counties
  3. A list of safe counties

You should build this analysis so that running it will extract the most current data straight from the NY-Times URL and the state name and date are parameters that can be changed allowing this report to be run for other states/dates. 

## Steps:

a. Start by reading in the data from the NY-Times URL with `read_csv` (make sure to attach the `tidyverse`). The data read from Github is considered our "raw data". Remember to always leave "raw-data-raw" and to generate meaningful subsets as you go. 

```{r}
library(tidyverse)
library(flextable)
data <- read_csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv')
```

b. Create an object called `my.date` and set it as "2022-02-01" - ensure this is a `date` object. 

c. Create a object called `my.state` and set it to "Colorado".  

::: {.callout-tip collapse="true"}
In R, `as.Date()` is a function used to convert character strings, numeric values, or other date-related objects into Date objects. It ensures that dates are stored in the correct format for date-based calculations and manipulations.

```{r}
txt <- "2025-02-15"
class(txt)
date_example <- as.Date(txt)
class(date_example)
```
:::

```{r}
my.date  <- as.Date("2022-02-01")
my.state <- "Colorado"
```

d. Start by making a subset that limits the data to Colorado (`filter`), and add a new column (`mutate`) with the daily _new cases_ using `diff/lag` by county (`group_by`). Do the same for _new deaths_. If lag is new to you, `lag` is a function that shifts a vector by a specified number of positions. The help file can be found with `?lag`.

(**Hint**: you will need some combination of `filter`, `group_by`, `arrange`, `mutate`, `diff/lag`, and `ungroup`)

```{r}
co_data <- data %>% 
  filter(state == my.state) %>% 
  group_by(county) %>% 
  mutate(new_cases = cases - lag(cases, n = 1),
         new_deaths = deaths - lag(deaths, n = 1)) %>%
  drop_na() %>% 
  ungroup()
```

f. Using your subset, generate (**2**) tables. The first should show the 5 counties with the most **CUMULATIVE** cases on you date of interest, and the second should show the 5 counties with the most **NEW** cases on that same date. Remember to use your `my.date` object as a proxy for today's date:

Your tables should have clear column names and descriptive captions.

(**Hint**: Use `flextable::flextable()` and `flextable::set_caption()`)

```{r}
today_date <- filter(co_data, date == my.date)

slice_max(today_date, n = 5, order_by = cases) %>% 
  select(county, cases) %>% 
  flextable() %>% 
  set_caption("Top 5 Counties by Cumulative Cases")

slice_max(today_date, n = 5, order_by = new_cases) %>% 
  select(county, state, new_cases) %>% 
  flextable() %>% 
  set_caption("Top 5 Counties by New Cases")
```

# **Question 2**: Evaluating Census Data (EDA)

Raw count data can be deceiving given the wide range of populations in Colorado countries. To help us normalize data counts, we need additional information on the population of each county. 

Population data is offered by the Census Bureau and can be found [here](https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/counties/totals/co-est2023-alldata.csv).

```{r}
pop_url <- 'https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/counties/totals/co-est2023-alldata.csv'
```

::: {.callout-tip}
## FIPs codes: Federal Information Processing
**How FIPS codes are used**\
  - FIPS codes are used in census products\
  - FIPS codes are used to identify geographic areas in files\
  - FIPS codes are used to identify American Indian, Alaska Native, and Native Hawaiian (AIANNH) areas \
  
**How FIPS codes are structured**\
  - The number of digits in a FIPS code depends on the level of geography\
  - State FIPS codes have two digits\
  - County FIPS codes have five digits, with the first two digits representing the state FIPS code\
:::

You notice that the COVID data provides a 5 digit character FIP code representing the state in the first 2 digits and the county in the last 3. In the population data, the STATE and COUNTY FIP identifiers are seperate columns. To make these compatible we need to create a FIP variable that concatinates the 2 digit STATE and the 3 digit COUNTY FIP.

::: {.callout-tip collapse="true"}

### Concatinating Strings.

In R, `paste()` provides a tool for concatenation. `paste()` can do two things:

1. concatenate values into one "string", e.g. where the argument `sep` specifies the character(s) to be used between the arguments to concatenate, or

```{r}
paste("Hello", "world", sep=" ")
```

2.  `collapse`  specifies the character(s) to be used between the elements of the vector to be collapsed.

```{r}
paste(c("Hello", "world"), collapse="-")
```

In R, it is so common to want to separate no separator (e.g. `
`paste("Hello", "world", sep="")`) that the short cut `paste0` exists:

```{r}
paste("Hello", "world", sep = "")
paste0("Hello", "world")
```
:::

## Steps:

a. Given the above URL, and guidelines on string concatenation, read in the population data and (1) create a five digit FIP variable and only keep columns that contain "NAME" or "2021" (remember the tidyselect option found with `?dplyr::select`). Additionally, remove all state level rows (e.g. COUNTY FIP == "000")

```{r}
cd <- read_csv(pop_url) %>% 
  filter(COUNTY != "000") %>% 
  mutate(fips = paste0(STATE, COUNTY)) %>% 
  select(STNAME, COUNTY, fips, contains("2021"))

joined_data <- inner_join(co_data, cd, by = ("fips")) %>%  
  glimpse()
```

b. Now, explore the data ... what attributes does it have, what are the names of the columns? Do any match the COVID data we have? What are the dimensions... In a few sentences describe the data obtained after modification:

(**Hint**: `names()`, `dim()`, `nrow()`, `str()`, `glimpse()`, `skimr`,...))

```{r}
names(joined_data)
dim(joined_data)
nrow(joined_data)
str(joined_data)
glimpse(joined_data)

## Based on some exploratory data structure analysis of this updated data set it is found that, after manipulation, this data frame has 26 columns and 49399 rows. Each column has its own unique name except for some overlap with the state name columns after joining the two data sets by the fips code column. 
```

# **Question 3**: Per Capita Summary

Join the population data to the Colorado COVID data and compute the per capita cumulative cases, per capita new cases, and per capita new deaths:

```{r}
joined_data <- joined_data %>%
  mutate(
    percap_cases = cases / `POPESTIMATE2021`,
    percap_new_cases = new_cases / `POPESTIMATE2021`,
    percap_new_deaths = new_deaths / `POPESTIMATE2021`
  )
```

Generate (**2**) new tables. The first should show the 5 counties with the most cumulative cases per capita on your date, and the second should show the 5 counties with the most **NEW** cases per capita on the same date. Your tables should have clear column names and descriptive captions.

(**Hint:** Use ``flextable::flextable()` and `flextable::set_caption()`)

```{r}
today_date <- filter(joined_data, date == my.date)

slice_max(today_date, n = 5, order_by = percap_cases) %>%
  select(county, percap_cases) %>%
  flextable() %>%
  set_caption("Top 5 Counties by Per Capita Cumulative Cases") %>% 
  set_header_labels(
    county = "County",
    percap_cases = "Cases Per Capita"
  )

slice_max(today_date, n = 5, order_by = percap_new_cases) %>% 
  select(county, percap_new_cases) %>% 
  flextable() %>% 
  set_caption("Top 5 Counties by Per Capita New Cases") %>% 
  set_header_labels(
    county = "County",
    percap_new_cases = "New Cases Per Capita"
  )

```

# **Question 4:** Rolling thresholds

Filter the merged COVID/Population data for Colorado to only include the last 14 days. *Remember this should be a programmatic request and not hard-coded*. 

Then, use the `group_by`/`summarize` paradigm to determine the total number of new cases in the last 14 days per 100,000 people. 

Print a table of the top 5 counties (consider `slice_max`), and, report the number of counties that meet the watch list condition: "More than 100 new cases per 100,000 residents over the past 14 days..."

(**Hint**: Dates are numeric in R and thus operations like `max` `min`, `-`, `+`, `>`, and` < ` work.)

```{r}

last_14_days <- joined_data %>%
  filter(date >= (my.date - 14) & date <= my.date)


rolling_summary <- last_14_days %>%
  group_by(county) %>%
  arrange(date) %>%
  mutate(
    roll_avg_new_cases = rollmean(new_cases, k = 14, fill = NA, align = "right"),
    roll_avg_new_deaths = rollmean(new_deaths, k = 14, fill = NA, align = "right")
  ) %>%
  ungroup()


top_cases <- rolling_summary %>%
  filter(!is.na(roll_avg_new_cases)) %>%
  group_by(county) %>%
  arrange(desc(date)) %>%  
  summarise(roll_avg_new_cases = last(roll_avg_new_cases), .groups = "drop") %>%
  slice_max(n = 5, order_by = roll_avg_new_cases)

top_deaths <- rolling_summary %>%
  filter(!is.na(roll_avg_new_deaths)) %>%
  group_by(county) %>%
  arrange(desc(date)) %>%
  summarise(roll_avg_new_deaths = last(roll_avg_new_deaths), .groups = "drop") %>%
  slice_max(n = 5, order_by = roll_avg_new_deaths)


top_cases %>%
  flextable() %>%
  set_caption("Top 5 Counties by 14-Day Rolling Average of New Cases") %>%
  set_header_labels(
    county = "County",
    roll_avg_new_cases = "14-Day Avg New Cases"
  )

top_deaths %>%
  flextable() %>%
  set_caption("Top 5 Counties by 14-Day Rolling Average of New Deaths") %>%
  set_header_labels(
    county = "County",
    roll_avg_new_deaths = "14-Day Avg New Deaths"
  )

```

# **Question 5**: Death toll

Given we are assuming it is February 1st, 2022. Your leadership has now asked you to determine what percentage of deaths in each county were attributed to COVID last year (2021). You eagerly tell them that with the current Census data, you can do this!

From previous questions you should have a `data.frame` with daily COVID deaths in Colorado and the Census based, 2021 total deaths. For this question, you will find the ratio of total COVID deaths per county (2021) of all recorded deaths. In a plot of your choosing, visualize all counties where COVID deaths account for 20% or more of the annual death toll.

::: {.callout-tip collapse="true"}
### Dates in R

To extract a element of a date object in R, the `lubridate` package (part of `tidyverse`) is very helpful:


```{r}
tmp.date = as.Date("2025-02-15")
lubridate::year(tmp.date)
lubridate::month(tmp.date)
lubridate::yday(tmp.date)
```
:::

```{r}
library(ggplot2)

covid_2021 <- joined_data %>%
  filter(year(date) == 2021)

county_covid_deaths <- covid_2021 %>%
  group_by(county) %>%
  summarize(
    total_covid_deaths = sum(new_deaths, na.rm = TRUE),
    population = first(POPESTIMATE2021),
    total_annual_deaths = first(DEATHS2021) 
  )

death_analysis <- county_covid_deaths %>%
  mutate(
    covid_death_ratio = total_covid_deaths / total_annual_deaths,
    covid_death_percentage = covid_death_ratio * 100
  ) %>%
  filter(covid_death_percentage >= 20) %>%
  arrange(desc(covid_death_percentage))

```

visualization
```{r}
ggplot(death_analysis, aes(x = reorder(county, covid_death_percentage), y = covid_death_percentage)) +
  geom_col(fill = "darkred", alpha = 0.8) +
  geom_text(aes(label = sprintf("%.1f%%", covid_death_percentage)), 
            hjust = -0.1, size = 3.5) +
  coord_flip() +
  labs(
    title = "Colorado Counties Where COVID Deaths Were â‰¥20% of Annual Deaths (2021)",
    subtitle = "Based on Census and NYT COVID-19 Data",
    x = NULL,
    y = "COVID Deaths as Percentage of Total Deaths",
    caption = paste("Data as of", format(as.Date("2022-02-01"), "%B %d, %Y"))
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.major.y = element_blank()
  )

```

# **Question 6**: Multi-state

Congratulations! You have been promoted to the National COVID-19 Task Force.As part of this exercise, you have been tasked with building analysis to compare states to each other.

In this question, we are going to look at the story of 4 states and the impact scale can have on data interpretation. The states include: **New York**, **Colorado**, **Alabama**, and **Ohio**. 

Your task is to make a _faceted_ bar plot showing the number of daily, **new** cases at the state level.

## Steps:

a. First, we need to `group/summarize` our county level data to the state level, `filter` it to the four states of interest, and calculate the number of daily new cases (`diff/lag`) and the 7-day rolling mean.

::: {.callout-tip collapse="true"}
### Rolling Averages

The `rollmean` function from the `zoo` package in R is used to compute the rolling (moving) mean of a numeric vector, matrix, or `zoo`/`ts` object.

`rollmean(x, k, fill = NA, align = "center", na.pad = FALSE)`\
  - `x`: Numeric vector, matrix, or time series.\
  - `k`: Window size (number of observations).\
  - `fill`: Values to pad missing results (default NA).\
  - `align`: Position of the rolling window ("center", "left", "right").\
  - `na.pad`: If TRUE, pads missing values with NA.\

#### Examples

1. Rolling Mean on a Numeric Vector
Since `align = "center"` by default, values at the start and end are dropped.

```{r}
library(zoo)

# Sample data
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

# Rolling mean with a window size of 3
rollmean(x, k = 3)
```


2. Rolling Mean with Padding
Missing values are filled at the start and end.

```{r}
rollmean(x, k = 3, fill = NA)
```

3. Aligning Left or Right
The rolling mean is calculated with values aligned to the left or right

```{r}
rollmean(x, k = 3, fill = NA, align = "left")
rollmean(x, k = 3, fill = NA, align = "right")
```
:::

**Hint:** You will need two `group_by` calls and the `zoo::rollmean` function.

```{r}
states_of_interest <- c("New York", "Colorado", "Alabama", "Ohio")

state_data <- data %>% 
  group_by(state, date) %>% 
  summarize(
    total_cases = sum(cases),
    total_deaths = sum(deaths),
    .groups = "drop"
  ) %>%
  filter(state %in% states_of_interest) %>%
  group_by(state) %>%
  arrange(date) %>%
  mutate(
    new_cases = total_cases - lag(total_cases, 1),
    new_cases = ifelse(new_cases < 0, NA, new_cases)
  ) %>%
  mutate(
    rolling_avg_cases = rollmean(new_cases, k = 7, fill = NA, align = "right")
  ) %>%
  ungroup()

```

b. Using the modified data, make a facet plot of the daily new cases and the 7-day rolling mean. Your plot should use compelling geoms, labels, colors, and themes.

```{r}
library(scales)

ggplot(state_data, aes(x = date)) +
  # Use columns for daily counts
  geom_col(aes(y = new_cases), fill = "lightblue", alpha = 0.6) +
  # Use lines for the rolling average (using linewidth instead of size)
  geom_line(aes(y = rolling_avg_cases), color = "darkblue", linewidth = 1) +
  # Create separate panels for each state
  facet_wrap(~state, scales = "free_y") +
  labs(
    title = "Daily New COVID-19 Cases by State",
    subtitle = "Daily counts with 7-day rolling average",
    x = "Date",
    y = "New Cases",
    caption = "Data source: New York Times COVID-19 data"
  ) +
  theme_minimal() +
  theme(
    strip.background = element_rect(fill = "lightgray"),
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b %Y") +
  scale_y_continuous(labels = scales::comma)
```

c. The story of raw case counts can be misleading. To understand why, lets explore the cases per capita of each state. To do this, join the state COVID data to the population estimates and calculate the $new cases / total population$. Additionally, calculate the 7-day rolling mean of the new cases per capita counts. **This is a tricky task and will take some thought, time, and modification to existing code (most likely)!**

**Hint**: You may need to modify the columns you kept in your original population data. Be creative with how you join data (inner vs outer vs full)! 

```{r}
state_pop <- cd %>%
  group_by(STNAME) %>%
  summarize(state_population = sum(POPESTIMATE2021)) %>%
  rename(state = STNAME)

state_percapita <- state_data %>%
  inner_join(state_pop, by = "state") %>%
  mutate(
    new_cases_percapita = new_cases / state_population * 100000,  # per 100,000 people
    rolling_avg_percapita = rollmean(new_cases_percapita, k = 7, fill = NA, align = "right")
  )


```

d. Using the per capita data, plot the 7-day rolling averages overlying each other (one plot) with compelling labels, colors, and theme. 

```{r}
ggplot(state_percapita, aes(x = date, y = rolling_avg_percapita, color = state)) +
  geom_line(linewidth = 1, alpha = 0.8) +
  labs(
    title = "COVID-19 Cases per 100,000 Residents",
    subtitle = "7-day rolling average by state",
    x = "Date",
    y = "New Cases per 100,000 People",
    color = "State",
    caption = "Data source: New York Times COVID-19 data"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  scale_x_date(date_breaks = "3 months", date_labels = "%b %Y") +
  scale_color_brewer(palette = "Set1")
```


e. Briefly describe the influence scaling by population had on the analysis? Does it make some states look better? Some worse? How so?

# scaling by population made it easier to see the individual peaks per state even if count were low, but led to some misunderstanding when graphs were compared to eachother. Without closely looking at the scales just visually, Colorado looked to have the same case counts as Alabama evn though on the scales Colorado was only at roughly 16,000 while Albama was at 20000. However, if there was no scaling by population, the states with lower case counts would barely be seen with enough clarity to compare state by state.
> ...

# **Question 7**: Space & Time

You've now been tasked with understanding how COVID has spread through time across the country. You will do this by calculating the Weighted Mean Center of the COVID-19 outbreak to better understand the movement of the virus through time. 

To do this, we need to join the COVID data with location information. I have staged the latitude and longitude of county centers [here](https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv). For reference, this data was processed like this:

```{r, eval = FALSE}
counties = USAboundaries::us_counties() %>% 
  dplyr::select(fips = geoid) %>% 
  sf::st_centroid() %>% 
  dplyr::mutate(LON = sf::st_coordinates(.)[,1], 
                LAT = sf::st_coordinates(.)[,2]) %>% 
  sf::st_drop_geometry()

write.csv(counties, "../resources/county-centroids.csv", row.names = FALSE)
```

Please read in the data (`readr::read_csv()`); and join it to your raw COVID-19 data using the `fips` attributes using the following URL:

```{r}
'https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv'
```

- The mean center of a set of spatial points is defined as the average X and Y coordinate. A weighted mean center can be found by weighting the coordinates by another variable such that:

$$X_{coord} = \sum{(X_{i} * w_{i})} / \sum(w_{i})$$
$$Y_{coord} = \sum{(Y_{i} * w_{i})}/ \sum(w_{i})$$

- For each date, calculate the Weighted Mean $X_{coord}$ and $Y_{coord}$ using the daily cumulative cases _and_ deaths as the respective $w_{i}$. 

```{r}
library(patchwork)
library(maps) 

centroids <- read_csv("https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv")

covid_joined <- data %>%
  left_join(centroids, by = "fips")

weighted_centers <- covid_joined %>%
  group_by(date) %>%
  summarize(
    X_cases = sum(LON * cases, na.rm = TRUE) / sum(cases, na.rm = TRUE),
    Y_cases = sum(LAT * cases, na.rm = TRUE) / sum(cases, na.rm = TRUE),
    X_deaths = sum(LON * deaths, na.rm = TRUE) / sum(deaths, na.rm = TRUE),
    Y_deaths = sum(LAT * deaths, na.rm = TRUE) / sum(deaths, na.rm = TRUE)
  ) %>%
  drop_na()

```

Make two plots next to each other (using `patchwork`) showing cases in navy and deaths in red. Once complete, describe the differences in the plots and what they mean about the spatial patterns seen with COVID impacts. These points should be plotted over a map of the USA states which can be added to a ggplot object with:

```{r, eval = FALSE}
borders("state", fill = "gray90", colour = "white")
```

(feel free to modify fill and **colour** (must be colour (see documentation)))

::: {.callout-tip collapse="true"}
### Multiplots

[`patchwork`](https://patchwork.data-imaginist.com/) is an R package designed for combining multiple `ggplot2` plots into a cohesive layout.

#### Key Features:
- **Simple Composition**: Use +, /, and | operators to arrange plots intuitively.\
- **Flexible Layouts**: Supports nesting, alignment, and customized positioning of plots.\
- **Annotation and Styling**: Add titles, captions, and themes across multiple plots.\

#### Example:

```{r}
library(patchwork)

p1 <- ggplot(mtcars, aes(mpg, hp)) + geom_point()
p2 <- ggplot(mtcars, aes(mpg, wt)) + geom_point()

p1 | p2  # Arrange side by side
```
This places p1 and p2 next to each other in a single figure.
:::

```{r}
usa_map <- borders("state", fill = "gray90", colour = "white")


p_cases <- ggplot(weighted_centers, aes(X_cases, Y_cases)) +
  usa_map +
  geom_path(color = "navy") +
  geom_point(color = "navy", size = 2) +
  labs(title = "Weighted Mean Center of COVID-19 Cases",
       x = "Longitude", y = "Latitude") +
  theme_minimal()

p_deaths <- ggplot(weighted_centers, aes(X_deaths, Y_deaths)) +
  usa_map +
  geom_path(color = "red") +
  geom_point(color = "red", size = 2) +
  labs(title = "Weighted Mean Center of COVID-19 Deaths",
       x = "Longitude", y = "Latitude") +
  theme_minimal()

p_cases | p_deaths
```

# **Question 8:** Trends

OK! This is a job well done. As your final task, your leadership has noticed that it is much easier to have a solid record of deaths, while a record of cases relies on testing protocols and availability. They ask you to explore the relationship between cases and deaths to see if deaths can be used as a proxy for cases. You will explore the relationship between cases and deaths along with other predictors of your chosing from the population data.

## Data Preparation

a. Let's start with the raw COVID dataset, and compute county level daily new cases and deaths (`lag`). Then, join it to the census data in order to use population data in the model. 

b. We are aware there was a strong seasonal component to the spread of COVID-19. To account for this, lets add a new column to the data for year (`lubridate::year()`), month (`lubridate::month()`), and `season` (`dplyr::case_when()`) which will be one of four values: "Spring" (Mar-May), "Summer" (Jun-Aug), "Fall" (Sep-Nov), or "Winter" (Dec - Jan) based on the computed Month.

c. Next, lets group the data by state, year, and season and summarize the total population, new cases, and new deaths per grouping. 

d. Given the case/death counts are not scaled by population, we expect that each will exhibit a right skew behavior (you can confirm this with density plots, shapiro.test, or histrograms). Given an assumption of linear models is normality in the data, let's apply a log transformation to cases, deaths, and population to normalize them. 

:::{.callout-note}
We know there are 0's in the data (cases/deaths), so we can add 1 to the data before taking the log. As the log of 0 is undefined, adding 1 ensures that the log of 0 is -Inf.

```{r}
log(0)
```
:::

```{r}
covid_trends <- data %>%
  arrange(fips, date) %>%
  group_by(fips) %>%
  mutate(
    new_cases = cases - lag(cases, default = 0),
    new_deaths = deaths - lag(deaths, default = 0)
  ) %>%
  ungroup()

covid_trends <- covid_trends %>%
  left_join(cd, by = "fips")

covid_trends <- covid_trends %>%
  mutate(
    year = year(date),
    month = month(date),
    season = case_when(
      month %in% 3:5 ~ "Spring",
      month %in% 6:8 ~ "Summer",
      month %in% 9:11 ~ "Fall",
      TRUE ~ "Winter"
    )
  )

covid_summary <- covid_trends %>%
  group_by(state, year, season) %>%
  summarize(
    total_population = sum(POPESTIMATE2021, na.rm = TRUE),
    total_cases = sum(new_cases, na.rm = TRUE),
    total_deaths = sum(new_deaths, na.rm = TRUE),
    .groups = "drop"
  )

covid_summary <- covid_summary %>%
  mutate(
    total_cases = ifelse(total_cases < 0 | is.na(total_cases), 0, total_cases),
    total_deaths = ifelse(total_deaths < 0 | is.na(total_deaths), 0, total_deaths),
    total_population = ifelse(total_population < 0 | is.na(total_population), median(total_population, na.rm = TRUE), total_population)
  )

covid_summary <- covid_summary %>%
  mutate(
    log_cases = log(total_cases + 1),
    log_deaths = log(total_deaths + 1),
    log_population = log(total_population + 1)
  )

ggplot(covid_summary, aes(log_cases)) + geom_histogram()
ggplot(covid_summary, aes(log_deaths)) + geom_histogram()
ggplot(covid_summary, aes(log_population)) + geom_histogram()

```

## Model Building

a. Once the data has been prepared, build a linear model (`lm`) to predict the log of cases using the log of deaths the log of population, and the season. Be sure to add an interaction term for population and deaths since they per capita realtionship is significant!

b. Once the model is built, summarize it (summary) and report the R-squared value and the p-value of the model. What does this mean for the value of its application?

```{r}
model <- lm(log_cases ~ log_deaths * log_population + season, data = covid_summary)

summary(model)

# The r^2 value was 0.3497 which is very low and the goodness of fit is poor. With that said though, the P value of this model is extremly low, meaning there is a statistically significant relationship between log population and log deaths.
```

# **Question 9:** Evaluation

Now that you have built a model, it is time to evaluate it. 

a. Start by using `broom::augment` to generate a data frame of predictions and residuals.

b. Lets, create a scatter plot of the predicted cases vs. the actual cases. Add a line of best fit to the plot, and make the plot as appealing as possible using `themes`, `scales_*`, and `labels.` Describe the realtionship that you see... are you happy with the model?

c. A final assumption of an appropriate model is that the residuals are normally distributed. Fortunatly `broom::augment` provides the .resid outputs for each feature. To visually check for residual normality, create a histogram of the residuals. Make the plot as appealing as possible using `themes`, `scales_*`, and `labels.` How does the distribution look? Was a linear model appropriate for this case?

```{r}
library(broom)

# Generate predictions and residuals
augmented_data <- augment(model, data = covid_summary)

# View the first few rows
head(augmented_data)



# Scatter plot of predicted vs. actual cases
ggplot(augmented_data, aes(x = .fitted, y = log_cases)) +
  geom_point(color = "blue") +  # Actual vs predicted points
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Line of best fit
  labs(x = "Predicted Log Cases", y = "Actual Log Cases", title = "Predicted vs. Actual Log Cases") +
  theme_minimal() +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    plot.title = element_text(size = 16, face = "bold")
  )

# Histogram of residuals
ggplot(augmented_data, aes(x = .resid)) +
  geom_histogram(binwidth = 0.1, fill = "skyblue", color = "black") +
  labs(x = "Residuals", y = "Frequency", title = "Residuals Histogram") +
  theme_minimal() +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    plot.title = element_text(size = 16, face = "bold")
  )

```

# Summary

And that's it! In this lab we have explored the COVID-19 data from the New York Times, wrangled it, and built a model to predict cases from deaths and population. This is a great example of how data science can be used to inform public health decisions.

We covered alot of technical tools as well spanning readr, dplyr, ggplot, lubridate, and more. We also used some more advanced tools like `zoo` for rolling averages and `broom` for model evaluation.

Through out the rest of class we will keep building on these skills and tools to become better data scientists.

# Rubric

- [ ] **Question 1**: Daily Summaries (10pts)
- [ ] **Question 2**: Evaluating Census Data (EDA) (10pts)
- [ ] **Question 3**: Per Capita Summary (10pts)
- [ ] **Question 4**: Rolling Thresholds (20pts)
- [ ] **Question 5**: Death toll (10pts)
- [ ] **Question 6**: Multi-state (20pts)
- [ ] **Question 7**: Space and Time (20pts)
- [ ] **Question 8**: Trends (20pts)
- [ ] **Question 9**: Evaluation (10pts)
- [ ] **Well Structured legible Qmd** (10pts)
- [ ] **Deployed as web page** (10pts)

**Total:** 150 points

# Submission

To submit your lab, you will deploy your knitted HTMLto a webpage hosted with GitHub pages. To do this:

 - Knit your lab document
 - Stage/commit/push your files
 - Activate Github Pages (GitHub --> Setting --> GitHub pages) 
 - If you followed the naming conventions in the "Set Up", your link will be available at: 
 
`https://USERNAME.github.io/csu-523c/lab-01.html`

Submit this URL in the appropriate Canvas dropbox. Also take a moment to update your personal webpage with this link and some bullet points of what you learned. While not graded as part of this lab, it will be eventually serve as extras credit!
